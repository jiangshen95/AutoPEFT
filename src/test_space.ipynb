{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LORA': [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32], 'LORA_LR': '1e-5', 'ADAPTER': [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128], 'ADAPTER_LR': '1e-6', 'EPOCHS': 3, 'PRUNE_EPOCHS': 1, 'PRUNE_TURN': 6, 'BACK_BONE': 'roberta-large'}\n",
      "{'DATASETS': [{'TASK_NAME': 'qnli', 'LOSS': 'cross_entropy', 'DATASET_NAME': 'glue'}, {'TASK_NAME': 'rte', 'LOSS': 'cross_entropy', 'DATASET_NAME': 'glue'}, {'TASK_NAME': 'wnli', 'LOSS': 'cross_entropy', 'DATASET_NAME': 'glue'}, {'TASK_NAME': 'cola', 'LOSS': 'cross_entropy', 'DATASET_NAME': 'glue'}, {'TASK_NAME': 'sst2', 'LOSS': 'cross_entropy', 'DATASET_NAME': 'glue'}, {'TASK_NAME': 'mrpc', 'LOSS': 'cross_entropy', 'DATASET_NAME': 'glue'}, {'TASK_NAME': 'qqp', 'LOSS': 'cross_entropy', 'DATASET_NAME': 'glue'}], 'PRUNE_METHODS': ['zeros', 'values_below_threshold', 'snip', 'minimum_weight', 'activation', 'gradient']}\n",
      "LORA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/autopeft/anaconda3/envs/autopeft/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of label classes: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/autopeft/AutoPEFT/src/run_model.py:345: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(\n",
      "/home/autopeft/AutoPEFT/src/run_model.py:363: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(\n",
      "/home/autopeft/AutoPEFT/src/run_model.py:331: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(\n"
     ]
    }
   ],
   "source": [
    "from src.run_model import PEFTModel\n",
    "from src.peft_search_space import PEFTSearchSpace\n",
    "from src.dataset_wrapper import PEFTDataset\n",
    "import torch\n",
    "from pruning_methods import prune_model\n",
    "from utils.gpu_memory_plot import get_free_gpu_memory\n",
    "from src.controllers.baseline_wrapper import baseline_wrapper_single, prune_wrapper_single, baseline_wrapper_double, prune_wrapper_double\n",
    "\n",
    "import yaml\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "logger = logging.getLogger('controller')\n",
    "path_method='../method_configs/adapter_lora.yaml'\n",
    "path_task='../task_configs/glue.yaml'\n",
    "with open(path_method, 'r') as file:\n",
    "    method_configs = yaml.safe_load(file)\n",
    "with open(path_task, 'r') as file:\n",
    "    task_configs = yaml.safe_load(file)\n",
    "\n",
    "logger.info(\n",
    "    f'Start exp for {path_task}:{task_configs}\\n{path_method}:{method_configs}')\n",
    "\n",
    "print(method_configs)\n",
    "print(task_configs)\n",
    "\n",
    "if 'LORA' in method_configs:\n",
    "    peft_type = 'LORA'\n",
    "else:\n",
    "    peft_type = 'ADAPTER'\n",
    "print(peft_type)\n",
    "\n",
    "ds_meta = task_configs['DATASETS'][0]\n",
    "dataset_name = ds_meta['DATASET_NAME']\n",
    "task_name = ds_meta['TASK_NAME']\n",
    "configs = deepcopy(method_configs)\n",
    "configs['LOSS'] = ds_meta['LOSS']\n",
    "\n",
    "dataset = PEFTDataset(\n",
    "    dataset_name, task_name, train_size=2000, test_size=400).get_dataset()\n",
    "\n",
    "model = PEFTModel(configs, dataset).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'params': ['roberta.encoder.layer.0.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.0.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.0.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.0.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.1.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.1.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.1.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.1.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.2.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.2.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.2.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.2.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.3.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.3.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.3.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.3.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.4.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.4.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.4.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.4.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.5.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.5.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.5.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.5.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.6.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.6.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.6.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.6.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.7.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.7.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.7.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.7.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.8.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.8.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.8.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.8.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.9.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.9.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.9.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.9.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.10.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.10.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.10.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.10.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.11.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.11.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.11.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.11.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.12.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.12.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.12.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.12.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.13.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.13.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.13.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.13.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.14.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.14.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.14.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.14.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.15.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.15.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.15.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.15.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.16.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.16.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.16.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.16.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.17.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.17.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.17.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.17.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.18.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.18.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.18.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.18.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.19.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.19.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.19.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.19.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.20.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.20.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.20.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.20.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.21.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.21.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.21.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.21.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.22.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.22.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.22.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.22.attention.self.value.loras.my_module.lora_B', 'roberta.encoder.layer.23.attention.self.query.loras.my_module.lora_A', 'roberta.encoder.layer.23.attention.self.query.loras.my_module.lora_B', 'roberta.encoder.layer.23.attention.self.value.loras.my_module.lora_A', 'roberta.encoder.layer.23.attention.self.value.loras.my_module.lora_B'], 'lr': 1e-05}, {'params': ['roberta.encoder.layer.0.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.0.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.0.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.0.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.1.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.1.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.1.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.1.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.2.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.2.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.2.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.2.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.3.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.3.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.3.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.3.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.4.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.4.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.4.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.4.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.5.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.5.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.5.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.5.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.6.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.6.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.6.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.6.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.7.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.7.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.7.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.7.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.8.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.8.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.8.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.8.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.9.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.9.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.9.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.9.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.10.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.10.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.10.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.10.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.11.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.11.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.11.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.11.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.12.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.12.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.12.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.12.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.13.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.13.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.13.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.13.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.14.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.14.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.14.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.14.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.15.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.15.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.15.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.15.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.16.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.16.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.16.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.16.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.17.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.17.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.17.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.17.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.18.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.18.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.18.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.18.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.19.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.19.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.19.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.19.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.20.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.20.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.20.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.20.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.21.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.21.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.21.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.21.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.22.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.22.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.22.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.22.output.adapters.my_module.adapter_up.bias', 'roberta.encoder.layer.23.output.adapters.my_module.adapter_down.0.weight', 'roberta.encoder.layer.23.output.adapters.my_module.adapter_down.0.bias', 'roberta.encoder.layer.23.output.adapters.my_module.adapter_up.weight', 'roberta.encoder.layer.23.output.adapters.my_module.adapter_up.bias'], 'lr': 1e-06}, {'params': ['heads.default.0.weight', 'heads.default.0.bias', 'heads.default.2.weight', 'heads.default.2.bias', 'heads.default.3.bias', 'heads.mytask.1.weight', 'heads.mytask.1.bias', 'heads.mytask.4.weight', 'heads.mytask.4.bias']}]\n",
      "201\n",
      "592\n"
     ]
    }
   ],
   "source": [
    "model.model.train()\n",
    "optimizer_grouped_parameters = [{\n",
    "    \"params\": [\n",
    "        name for name, param in model.model.named_parameters() if \"lora\" in name\n",
    "    ],\n",
    "    \"lr\": 1e-5,\n",
    "}, {\n",
    "    \"params\": [\n",
    "        name for name, param in model.model.named_parameters()\n",
    "        if \"adapter\" in name\n",
    "    ],\n",
    "    \"lr\": 1e-6,\n",
    "}, {\n",
    "    \"params\": [\n",
    "        name for name, param in model.model.named_parameters()\n",
    "        if 'heads' in name\n",
    "    ],\n",
    "    \"lr\": 1e-5\n",
    "}]\n",
    "print(optimizer_grouped_parameters)\n",
    "print(sum([len(i['params']) for i in optimizer_grouped_parameters]))\n",
    "print(len(list(model.model.parameters())))\n",
    "# optimizer = torch.optim.AdamW(optimizer_grouped_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pruning_methods import prune_model\n",
    "idx, idt = prune_model(\n",
    "    model.model,\n",
    "    task_name='my_module',\n",
    "    opts=['lora', 'adapter'],\n",
    "    p_method='gradient',\n",
    "    top_p=12,\n",
    "    print_names=True,\n",
    "    gradients=gradients)\n",
    "logger.info(f'Pruned layer: {idx, idt}')\n",
    "search_list[int(idx)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n",
      "422\n"
     ]
    }
   ],
   "source": [
    "params_need_record = [\n",
    "    param for name, param in model.model.named_parameters()\n",
    "    if param.requires_grad\n",
    "]\n",
    "print(len(params_need_record))\n",
    "print(len(list(model.model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m groups\u001b[38;5;241m=\u001b[39m [name \u001b[38;5;28;01mfor\u001b[39;00m name, para \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mnamed_parameters()]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(groups)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "groups= [name for name, para in model.model.named_parameters()]\n",
    "print(groups)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autopeft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
